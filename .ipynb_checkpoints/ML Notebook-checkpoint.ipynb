{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared 0.320639530846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('wine.csv', sep = ';')\n",
    "X = df[list(df.columns)[:-1]]\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_prediction = regressor.predict(X_test)\n",
    "print 'R-squared', regressor.score(X_test,y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32% of the variance in the test set is explained by the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.290041628842 [ 0.13200871  0.31858135  0.34955348  0.369145    0.2809196 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('wine.csv', sep = ';')\n",
    "X = df[list(df.columns)[:-1]]\n",
    "y = df['quality']\n",
    "\n",
    "regressor = LinearRegression()\n",
    "scores = cross_val_score(regressor, X,y,cv=5 )\n",
    "print scores.mean(), scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 times cross-validation. mean value is 29% This suggest this is a bettter value to use than 32%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'   \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 unique words, this is the vocabulary. each  word isgiven a binary value.\n",
    "\n",
    "UNC is the first element and present in the first document = 1\n",
    "\n",
    "game is not present in the first document so = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'      \n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top row of binary, 1 or 0 for unquie words. prder respesented by number after words.\n",
    "\n",
    "same with bottom row, compaire each unquie word with word in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game' ,\n",
    "    'I ate a Sandwich'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now have ten unique words AND last document clearly has less in common than first two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[ 2.44948974]]\n",
      "Distance between 1st and 3nd documents: [[ 2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[ 2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "counts = [\n",
    "     [0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
    "     [0, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0, 1, 0 ,0]       \n",
    "]\n",
    "\n",
    "print 'Distance between 1st and 2nd documents:', euclidean_distances(counts[0], counts[1])\n",
    "print 'Distance between 1st and 3nd documents:', euclidean_distances(counts[0], counts[2])\n",
    "print 'Distance between 2nd and 3rd documents:', euclidean_distances(counts[1], counts[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the euclidean distance between first sentance and secound is less that between the third and either of the other two sentances. This shows, emprically, that the third sentance is less like the first two.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a Sandwich'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop_words='english' removes words such as 'a' and 'the' as well as some verbs, 'do','will'  etc. these are not considered important in terms of sentace meaning. We now have 8 elements in the arrays, not ten. Making comparision eaisier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 1 1]]\n",
      "{u'ate': 0, u'sandwich': 2, u'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'He ate the sandwich',\n",
    "    'Every sandwich was eaten by him',\n",
    " \n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, stop_words='english')\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n','v']\n",
    "\n",
    "corpus = [\n",
    "    'He ate the sandwich',\n",
    "    'Every sandwich was eaten by him',\n",
    " \n",
    "]\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n','v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "\n",
    "print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
